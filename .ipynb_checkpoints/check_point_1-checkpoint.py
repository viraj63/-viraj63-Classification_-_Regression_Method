{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0414eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Step 1 - Import Libraries.\n",
    "# \n",
    "# Import required libraries (not allowed: scikit-learn or any other libraries with inbuilt functions that help to implement ML methods).\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#all required libraries imported \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# ## Step 2 - Reading the data and printing the statistics.\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#reading the File\n",
    "#pd.set_option('display.max_rows', None)\n",
    "data=pd.read_csv(\"C:/Users/viraj/Code_a/ML_Prof_Alina/Assignment_1/datasets (1)/datasets/penguins.csv\")\n",
    "\n",
    "\n",
    "# Read, preprocess, and print the main statistics about the dataset (you can reuse\n",
    "# your code from Assignment 0 with a proper citation)\n",
    "# \n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#showcasing the first 5  rows of the dataset\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#checking the null values\n",
    "data.isnull().sum() \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#deleting the rows having NAN values\n",
    "data=data.dropna()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#showcasing the Datatype of elements of the columns\n",
    "data.info()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#The main statistics of the dataSet\n",
    "data.describe()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "d1=data.species.describe()   \n",
    "d2=data.island.describe() \n",
    "d3=data.sex.describe() \n",
    "print(d1,d2,d3)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#printing the column names\n",
    "for col in data.columns:\n",
    "    print(col)  \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## STEP 3 - Convert features with string datatype to categorical (species, island, sex).\n",
    "# \n",
    "# Example: suppose you have a dataset that contains information about movies,\n",
    "# with the following features: title (string), director (string), genre (string). You need\n",
    "# to convert these features of string datatype to categorical features. This can be\n",
    "# done by assigning a unique numerical value to each unique string value in each\n",
    "# categorical feature.\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Here Species,islands and sex has datatype as string \n",
    "\n",
    "data[['species','island','sex']] = data[['species','island','sex']].astype('category')\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "data.info()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# Converting Categorical value into numerical value\n",
    "data['sex'] = pd.factorize(data['sex'])[0]\n",
    "data['island'] = pd.factorize(data['island'])[0]\n",
    "data['species'] = pd.factorize(data['species'])[0]\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# Here we could see the datatype has been changed of the following \n",
    "data.info() \n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "print(data.dtypes)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "data.shape\n",
    "\n",
    "\n",
    "# ## 4. Normalize non-categorical features (bill_length_mm, bill_depth_mm,flipper_length_mm, body_mass_g).\n",
    "# \n",
    "# a. Find the min and max values for each column.\n",
    "# \n",
    "# b. Rescale dataset columns to the range from 0 to 1\n",
    "# \n",
    "# \n",
    "# Why do we do this? Normalization is to transform features to be on a similar\n",
    "# scale. This improves the performance and training stability of the model.\n",
    "# \n",
    "# ###### Note: normalize() is not allowed as it is a part of scikit-learn library.\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "data.head()\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#a. Find the min and max values for each column.\n",
    "\n",
    "bill_length_mm_min , bill_length_mm_max = min(data['bill_length_mm']),max(data['bill_length_mm'])\n",
    "\n",
    "bill_depth_mm_min ,  bill_depth_mm_max = min(data['bill_depth_mm']) ,max(data['bill_depth_mm'])\n",
    "\n",
    "flipper_length__mm_min ,flipper_length_mm_max = min(data['flipper_length_mm']) , max(data['flipper_length_mm'])\n",
    " \n",
    "body_mass_g_min , body_mass_g_max = min(data['body_mass_g']) , max(data['body_mass_g'])\n",
    " \n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "data1=data\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "#b. Rescale dataset columns to the range from 0 to 1\n",
    "\n",
    "data1['bill_length_mm'] = (data1['bill_length_mm'] - bill_length_mm_min) / (bill_length_mm_max - bill_length_mm_min)\n",
    "data1['bill_depth_mm']  = (data1['bill_depth_mm']  - bill_depth_mm_min)  / (bill_depth_mm_max  - bill_depth_mm_min)\n",
    "data1['flipper_length_mm'] = (data1['flipper_length_mm'] - flipper_length__mm_min)/(flipper_length_mm_max - flipper_length__mm_min)\n",
    "data1['body_mass_g'] = (data1['body_mass_g'] - body_mass_g_min)/(body_mass_g_max - body_mass_g_min)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "data1\n",
    "\n",
    "\n",
    "# ## 5. Choose your target Y. For this dataset, there are several options:\n",
    "# a. We can use a binary classifier to predict which gender a penguin belongs to (female or male). In this case, column sex can be used as Y (target)\n",
    "# \n",
    "# \n",
    "# b. We can use a binary classifier to predict if a penguin‚Äôs location is Torgersen island or not. In this case, column island can be used as Y (target) \n",
    "# \n",
    "# \n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "# Step 5 - Choosing target\n",
    "# Here we have chosen sex as the target, rest all are the inputs some of them are dropped.\n",
    "\n",
    "\n",
    "# ## STEP 6 - Create the data matrices for X (input) and Y (target) in a shape,X = ùëÅ x ùëë and Y = ùëÅ x 1, were ùëÅ is a number of data samples and ùëë has a number of features. \n",
    "# \n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "data1\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "data1['sex'] = pd.factorize(data1['sex'])[0]\n",
    "data1['island'] = pd.factorize(data1['island'])[0]\n",
    "data1['species'] = pd.factorize(data1['species'])[0]\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "data1.info()\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "import random\n",
    "data1 = data1.sample(frac = 1)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "data2_except_sex_X=data1[['bill_length_mm','bill_depth_mm','flipper_length_mm','body_mass_g']]\n",
    "data2_except_sex_X.shape\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "data3_sex_column_Y=data1['sex']\n",
    "print(data3_sex_column_Y.shape)\n",
    "data3_sex_column_Y=data3_sex_column_Y.astype(int)\n",
    "#print(data3_sex_column_Y.dtype)\n",
    "\n",
    "\n",
    "# ## Step 7 - Divide the dataset into training and test, as 80% training, 20% testing dataset.\n",
    "# \n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(46)\n",
    "\n",
    "train_size = int(len(data2_except_sex_X) * 0.8)\n",
    "\n",
    "X_train = data2_except_sex_X[0:train_size]\n",
    "Y_train = data3_sex_column_Y[0:train_size]\n",
    "\n",
    "X_test = data2_except_sex_X[train_size : ]\n",
    "Y_test = data3_sex_column_Y[train_size : ]\n",
    "\n",
    "\n",
    "# ## Step 8 -Print the shape of your X_train, y_train, X_test, y_test\n",
    "# \n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "\n",
    "\n",
    "# ## Step 9 - Code for the Logistic Regression using the recommended structure of the code for defining logistic regression:\n",
    "# \n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "import math\n",
    "class LogitRegression():\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, iterations = 10000):\n",
    "        # Takes as an input hyperparameters: learning rate and the number of iterations\n",
    "        # Has weights and bias also.\n",
    "        # We have self.losses to append the losses.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.losses = []\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Defining the sigmoid function.\n",
    "        sigma = 1/(1 + np.exp(-x))\n",
    "        return sigma\n",
    "    \n",
    "    def cost(self, y, y_pred):\n",
    "        # Defining the loss function.\n",
    "        N = len(y)\n",
    "        # Formula for the loss.\n",
    "        cost = -(1/N) * (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "        return cost\n",
    "        \n",
    "    def gradient(self, n, X, y, y_pred):\n",
    "        # Defining gradient function.\n",
    "        delta = y_pred - y\n",
    "        # Formula for the weights.\n",
    "        dw = (1 / n) * np.dot(np.transpose(X), (delta))\n",
    "        # Formula for the bias.\n",
    "        db = (1 / n) * np.sum(delta)\n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n, no_feature  = X.shape\n",
    "        # Assigning random weights and bias zero.\n",
    "        self.weights = np.random.uniform(0, 1, 4)\n",
    "        self.bias = 0\n",
    "        for i in range(self.iterations):\n",
    "            # Using sigmoid defined to get prediction.\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "            # Getting weights and bias.\n",
    "            dw, db = self.gradient(n, X, y, y_pred)\n",
    "            # Calculating loss\n",
    "            loss = self.cost(y, y_pred)\n",
    "            print(f\"Iteration {i}: loss = {np.mean(loss)}\")\n",
    "            # Appending loss to the list.\n",
    "            self.losses.append(loss)\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate*dw\n",
    "            self.bias -= self.learning_rate*db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "        y_sex = []\n",
    "        for i in y_pred:\n",
    "            # Appending 1 if i is greater than or equal to 0.5.\n",
    "            if i >= 0.5:\n",
    "                y_sex.append(1)\n",
    "            # Appending 0 if i is less than 0.5.\n",
    "            else:\n",
    "                y_sex.append(0)\n",
    "        weight = self.weights\n",
    "        return y_sex, weight\n",
    "\n",
    "\n",
    "# ## Step 10 -  Train the model:\n",
    "# a. Define a model by calling LogitRegression class and passing\n",
    "# hyperparameters, e.g.\n",
    "# model = LogitRegression(learning_rate, iterations)\n",
    "# b. Train the model, by calling fit function and passing your training dataset,\n",
    "# e.g\n",
    "# model.fit(X_train, y_train)\n",
    "# c. Suggested hyperparameters:\n",
    "# Note: You can try different learning rates and number of iterations to\n",
    "# improve your accuracy (accuracy of greater than 64% is expected)\n",
    "# learning_rate=1e-3\n",
    "# iterations=100000\n",
    "# weights = np.random.uniform(0, 1)\n",
    "# \n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model = LogitRegression(0.001, 100000)\n",
    "model.fit(X_train, Y_train)\n",
    "plt.plot(model.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## Step 11 - Make a prediction on test dataset by counting how many correct/incorrect predictions your model makes and print your accuracy\n",
    "# \n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "y_pred, weight = model.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "accuracy_lst = []\n",
    "weight_lst = []\n",
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)\n",
    "\n",
    "\n",
    "# ## Fitting for different values of learning rate and iterations.\n",
    "\n",
    "# ### Case 2\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model1 = LogitRegression(0.0005, 100000)\n",
    "model1.fit(X_train, Y_train)\n",
    "plt.plot(model1.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y_pred, weight = model1.predict(X_test)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "print(weight)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)\n",
    "print(weight_lst)\n",
    "\n",
    "\n",
    "# ### Case 3\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model2 = LogitRegression(0.1, 10000)\n",
    "model2.fit(X_train, Y_train)\n",
    "plt.plot(model2.losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss during training')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "y_pred, weight = model2.predict(X_test)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == Y_test)\n",
    "print(accuracy)\n",
    "print(weight)\n",
    "accuracy_lst.append(accuracy)\n",
    "weight_lst.append(weight)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "max_accuracy = max(accuracy_lst)\n",
    "i = accuracy_lst.index(max_accuracy)\n",
    "weight_max = weight_lst[i]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump( weight_max, open( \"weight_pickle.p\", \"wb\" ) )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "weight_pickle = pickle.load( open( \"weight_pickle.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(f'Weights for best accuracy {weight_pickle}')\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "# Part II: Linear Regression \n",
    "\n",
    "# In this part, we implement linear regression model and apply this model to solve a\n",
    "# problem based on the real-world dataset.\n",
    "# Datasets that can be used for this part (provided in the zip folder):\n",
    "# ‚Ä¢ Flight price prediction dataset\n",
    "# ‚Ä¢ Breeding Bird Atlas\n",
    "# ‚Ä¢ Diamond dataset (Note: x, y and z columns refer to length, width, and depth\n",
    "# respectively)\n",
    "# ‚Ä¢ Emissions by Country dataset\n",
    "# ‚Ä¢ Epicurious ‚Äì Recipes with Rating and nutrition\n",
    "# Implement linear regression using the ordinary least squares (OLS) method to perform\n",
    "# direct minimization of the squared loss function.\n",
    "# ùêΩ(ùíò) =\n",
    "# 1\n",
    "# 2\n",
    "# ‚àë(ùë¶ùëñ ‚àí ùë§\n",
    "# ùëáùë•ùëñ\n",
    "# )\n",
    "# 2\n",
    "# ùëÅ\n",
    "# ùëñ=1\n",
    "# In matrix-vector notation, the loss function can be written as:\n",
    "# ùêΩ(ùíò) =\n",
    "# 1\n",
    "# 2\n",
    "# ‚àë(ùíö ‚àí ùëøùíò)\n",
    "# ùëá\n",
    "# (ùíö ‚àí ùëøùíò)\n",
    "# ùëÅ\n",
    "# ùëñ=1\n",
    "# where ùëø is the input data matrix, ùíö is the target vector, and ùíò is the weight vector for\n",
    "# regression.\n",
    "\n",
    "# \n",
    "# ##### Step_1_Select one dataset from the list provided above. The datasets are located in the folder ‚Äúdataset‚Äù, use only the dataset provided in the folder\n",
    "# \n",
    "\n",
    "# In[225]:\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"C:/Users/viraj/Code_a/ML_Prof_Alina/Assignment_1/datasets (1)/datasets/diamond.csv\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
